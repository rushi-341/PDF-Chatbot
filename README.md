# ğŸ’¬ Chat with PDF using LLaMA-3.1 (Groq + LangChain)

This project allows users to chat with their PDF files using natural language queries.
It is built using LangChain, LLaMA-3.1 (hosted on Groq) for large language model inference, FAISS for vector-based document retrieval, and Streamlit for an interactive web interface.

The application follows a Retrieval-Augmented Generation (RAG) approach, enabling accurate and context-aware answers strictly based on the content of the uploaded PDF documents.
---
ğŸš€ Features

ğŸ“„ Upload and process multiple PDF files

ğŸ” Ask questions and receive detailed, context-aware answers from the uploaded PDFs

ğŸ¤– Powered by LLaMA-3.1 (hosted on Groq) for fast and high-quality LLM responses

ğŸ”— Uses LangChain + FAISS for efficient vector-based document retrieval

ğŸ§  Semantic chunking and embeddings using HuggingFace sentence transformers for accurate context matching

â˜ï¸ Fully deployable on Streamlit Cloud (no local models required)

## ğŸ› ï¸ Tech Stack

Python

Streamlit

LangChain

Groq API (LLaMA-3.1)

FAISS (vector database)

HuggingFace Sentence Transformers (embeddings)

PyPDF

python-dotenv

---

## ğŸ“‚ Project Structure

```bash
chat_with_pdf/
â”œâ”€â”€ chat.py                # Main Streamlit application
â”œâ”€â”€ faiss_index/           # FAISS vector store (auto-created after processing PDFs)
â”œâ”€â”€ .env                   # Stores Groq API key (local use only, not committed)
â”œâ”€â”€ .gitignore             # Excludes .env and other sensitive files
â”œâ”€â”€ README.md              # Project documentation
â””â”€â”€ requirements.txt       # Python dependencies
